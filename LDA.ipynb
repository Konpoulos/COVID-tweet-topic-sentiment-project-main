{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import spacymoji\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "import gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data combined with scraped data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1.240739e+18</td>\n",
       "      <td>Well one positive thing about this Corona Viru...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1.240739e+18</td>\n",
       "      <td>Due to the growing health concerns of the Coro...</td>\n",
       "      <td>0.279762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1.240739e+18</td>\n",
       "      <td>Hopefully Soon Ass This Corona Virus ðŸ¦  Bullshi...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1.240739e+18</td>\n",
       "      <td>Friends donâ€™t let friends get fat during Coron...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1.240740e+18</td>\n",
       "      <td>#Repost cristi_christensen\\nãƒ»ãƒ»ãƒ»\\nTake that Cor...</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date            id  \\\n",
       "20  2020-03-20  1.240739e+18   \n",
       "21  2020-03-20  1.240739e+18   \n",
       "22  2020-03-20  1.240739e+18   \n",
       "23  2020-03-20  1.240739e+18   \n",
       "24  2020-03-20  1.240740e+18   \n",
       "\n",
       "                                                tweet  sentiment  \n",
       "20  Well one positive thing about this Corona Viru...   0.000000  \n",
       "21  Due to the growing health concerns of the Coro...   0.279762  \n",
       "22  Hopefully Soon Ass This Corona Virus ðŸ¦  Bullshi...   0.000000  \n",
       "23  Friends donâ€™t let friends get fat during Coron...   0.000000  \n",
       "24  #Repost cristi_christensen\\nãƒ»ãƒ»ãƒ»\\nTake that Cor...   0.187500  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"Data/all_tweets_sorted_fixed_date.csv\", index_col=[0])\n",
    "print(\"Original data combined with scraped data\")\n",
    "all_data.iloc[range(20, 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = all_data[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb059c16f09a4ca182dd11e4ef7481f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7395c0f946d748fd9e76d3a283dc0ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=127128.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "docs = [post for post in tqdm(nlp.pipe(tweets, disable=[\"ner\", \"parser\"]))]\n",
    "lemmas = [[t.lemma_ for t in doc if not t.is_punct and not t.is_stop and t.pos_ in [\"NOUN\", \"VERB\", \"ADJ\"]] for doc in tqdm(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134ebfc7a91b4437a9a21948c8fbe71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=127128.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemmas_nouns = [[t.lemma_ for t in doc if not t.is_punct and not t.is_stop and t.pos_ in [\"NOUN\"]] for doc in tqdm(docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip code below till loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_DF = 20 # minium document frequency\n",
    "# MAX_DF = 0.6 # maximum document frequency\n",
    "\n",
    "# dictionary = Dictionary(lemmas) # get the vocabulary\n",
    "# dictionary.filter_extremes(no_below=MIN_DF, \n",
    "#                            no_above=MAX_DF)\n",
    "# corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "\n",
    "# PATH_TO_MALLET = \"mallet-2.0.8/bin/mallet\"\n",
    "# N_TOPICS = 20\n",
    "# N_ITERATIONS = 10000\n",
    "\n",
    "# # TAKES LONG!\n",
    "# lda = LdaMallet(PATH_TO_MALLET,\n",
    "#                 corpus=corpus,\n",
    "#                 id2word=dictionary,\n",
    "#                 num_topics=N_TOPICS,\n",
    "#                 iterations=N_ITERATIONS)\n",
    "\n",
    "# lda.save(f\"Models/lda{N_TOPICS}_{N_ITERATIONS}i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = Dictionary(lemmas_nouns) # get the vocabulary\n",
    "# dictionary.filter_extremes(no_below=MIN_DF, \n",
    "#                            no_above=MAX_DF)\n",
    "# corpus = [dictionary.doc2bow(text) for text in lemmas_nouns]\n",
    "\n",
    "# PATH_TO_MALLET = \"mallet-2.0.8/bin/mallet\"\n",
    "# N_TOPICS = 20\n",
    "# N_ITERATIONS = 10000\n",
    "\n",
    "# # TAKES LONG!\n",
    "# lda_nouns = LdaMallet(PATH_TO_MALLET,\n",
    "#                 corpus=corpus,\n",
    "#                 id2word=dictionary,\n",
    "#                 num_topics=N_TOPICS,\n",
    "#                 iterations=N_ITERATIONS)\n",
    "\n",
    "# lda_nouns.save(f\"Models/lda{N_TOPICS}_{N_ITERATIONS}i_nouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda = LdaMallet.load(f\"Models/lda{N_TOPICS}_{N_ITERATIONS}i_nouns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = [35, 50]\n",
    "NOUNS_ONLY = [False]\n",
    "N_ITERATIONS = [10000]\n",
    "MIN_DF = 20 # minium document frequency\n",
    "MAX_DF = 0.6 # maximum document frequency  \n",
    "PATH_TO_MALLET = \"mallet-2.0.8/bin/mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: Models/lda_35t_10000i\n",
      "DONE: Models/lda_50t_10000i\n",
      "COMPLETELY DONE\n"
     ]
    }
   ],
   "source": [
    "for n_topics in N_TOPICS:\n",
    "    for n_iterations in N_ITERATIONS:\n",
    "        for nouns_only in NOUNS_ONLY:\n",
    "            if nouns_only == False:\n",
    "                nouns_only = \"\"\n",
    "                lemmas_ = lemmas\n",
    "            if nouns_only == True:\n",
    "                nouns_only = \"_nouns\"\n",
    "                lemmas_ = lemmas_nouns\n",
    "            save_name = f\"Models/lda_{n_topics}t_{n_iterations}i\"+nouns_only\n",
    "            if not os.path.isfile(save_name):\n",
    "                dictionary = Dictionary(lemmas_) # get the vocabulary\n",
    "                dictionary.filter_extremes(no_below=MIN_DF, \n",
    "                                           no_above=MAX_DF)\n",
    "                corpus = [dictionary.doc2bow(text) for text in lemmas_]\n",
    "\n",
    "                # TAKES LONG!\n",
    "                lda = LdaMallet(PATH_TO_MALLET,\n",
    "                                corpus=corpus,\n",
    "                                id2word=dictionary,\n",
    "                                num_topics=n_topics,\n",
    "                                iterations=n_iterations)\n",
    "\n",
    "                lda.save(save_name)\n",
    "            print(\"DONE: \"+save_name)\n",
    "print(\"COMPLETELY DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for n_topics in N_TOPICS:\n",
    "#     for n_iterations in N_ITERATIONS:\n",
    "#         for nouns_only in NOUNS_ONLY:\n",
    "#             if nouns_only == False:\n",
    "#                 nouns_only = \"\"\n",
    "#             if nouns_only == True:\n",
    "#                 nouns_only = \"_nouns\"\n",
    "#             save_name = f\"Models/lda_{n_topics}t_{n_iterations}i\"+nouns_only\n",
    "#             print(\"\\n\"+save_name)\n",
    "#             lda = LdaMallet.load(save_name)\n",
    "#             top_10 = []\n",
    "#             for topic in range(n_topics):\n",
    "#                 words = lda.show_topic(topic, 10)\n",
    "#                 topic_n_words = ', '.join([word[0] for word in words])\n",
    "#                 temp_top_10 = 'Topic {}: {}'.format(str(topic), topic_n_words)\n",
    "#                 top_10.append(temp_top_10)\n",
    "#                 print(temp_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10 = []\n",
    "# for topic in range(N_TOPICS):\n",
    "#     words = lda.show_topic(topic, 10)\n",
    "#     topic_n_words = ', '.join([word[0] for word in words])\n",
    "#     temp_top_10 = 'Topic {}: {}'.format(str(topic), topic_n_words)\n",
    "#     top_10.append(temp_top_10)\n",
    "#     print(temp_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10 = []\n",
    "# for topic in range(N_TOPICS):\n",
    "#     words = lda_nouns.show_topic(topic, 10)\n",
    "#     topic_n_words = ', '.join([word[0] for word in words])\n",
    "#     temp_top_10 = 'Topic {}: {}'.format(str(topic), topic_n_words)\n",
    "#     top_10.append(temp_top_10)\n",
    "#     print(temp_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_distributions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all_data with topics per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_docs = lda.load_document_topics()\n",
    "# topic_distributions = pd.DataFrame([[x[1] for x in doc] for doc in transformed_docs], \n",
    "#                                    columns=[i for i in range(N_TOPICS)])\n",
    "# topic_columns = [\"topic_\"+str(col) for col in topic_distributions.columns]\n",
    "# topic_distributions.columns = topic_columns\n",
    "# all_data_topic = all_data.join(topic_distributions).reset_index(drop=True)\n",
    "# all_data_topic.to_csv(\"Data/tweet_topic.csv\", index=False)\n",
    "# all_data_topic.iloc[range(10, 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_docs = lda_nouns.load_document_topics()\n",
    "# topic_distributions = pd.DataFrame([[x[1] for x in doc] for doc in transformed_docs], \n",
    "#                                    columns=[i for i in range(N_TOPICS)])\n",
    "# topic_columns = [\"topic_\"+str(col) for col in topic_distributions.columns]\n",
    "# topic_distributions.columns = topic_columns\n",
    "# all_data_topic = all_data.join(topic_distributions).reset_index(drop=True)\n",
    "# all_data_topic.to_csv(\"Data/tweet_nouns_topic.csv\", index=False)\n",
    "# all_data_topic.iloc[range(10, 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models/lda_35t_10000i\n",
      "\n",
      "Models/lda_50t_10000i\n"
     ]
    }
   ],
   "source": [
    "for n_topics in N_TOPICS:\n",
    "    for n_iterations in N_ITERATIONS:\n",
    "        for nouns_only in NOUNS_ONLY:\n",
    "            if nouns_only == False:\n",
    "                nouns_only = \"\"\n",
    "            if nouns_only == True:\n",
    "                nouns_only = \"_nouns\"\n",
    "            save_name = f\"Models/lda_{n_topics}t_{n_iterations}i\"+nouns_only\n",
    "            print(\"\\n\"+save_name)\n",
    "            lda = LdaMallet.load(save_name)\n",
    "            \n",
    "            transformed_docs = lda.load_document_topics()\n",
    "            topic_distributions = pd.DataFrame([[x[1] for x in doc] for doc in transformed_docs], \n",
    "                                               columns=[i for i in range(n_topics)])\n",
    "            topic_columns = [\"topic_\"+str(col) for col in topic_distributions.columns]\n",
    "            topic_distributions.columns = topic_columns\n",
    "            all_data_topic = all_data.join(topic_distributions).reset_index(drop=True)\n",
    "            all_data_topic.to_csv(f\"Data/tweet_topic_{n_topics}t_{n_iterations}i\"+nouns_only+\".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
